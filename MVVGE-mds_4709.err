2022-11-01 11:52:31.627235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:31.627275: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-11-01 11:52:33.943240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943323: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943375: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943425: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943483: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943639: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/uw/slurm/22.05.2/lib:/opt/uw/slurm/22.05.2/lib/slurm:/usr/local/lib:/opt/uw/openmpi/4.1.4/lib:/opt/uw/openmpi/4.1.4/lib/pmix:/opt/uw/openmpi/4.1.4/lib/openmpi:/opt/uw/slurm/22.05.2/lib/slurm:/opt/uw/slurm/22.05.2/lib
2022-11-01 11:52:33.943666: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Traceback (most recent call last):
  File "main.py", line 370, in <module>
    main()
  File "main.py", line 246, in main
    dict_ensemble = evaluate_ga('/csv/' + name[:-4] + '.csv', labels, train_data, test_data, sys.argv[3])
  File "/mnt/campus/math/cluster/research/slurm-pr1/work_root/mmalekis/MVVGE/src/ga/evaluate_ga.py", line 433, in evaluate_ga
    , train_x_lr_st_reduction, train_x_lr_reduction, train_y_lr_reduction, test_x_lr_st_reduction, test_x_lr_reduction, test_y_lr_reduction, reduction_variance, reduction_variance_ratio = process.mds(
  File "/mnt/campus/math/cluster/research/slurm-pr1/work_root/mmalekis/MVVGE/src/data_handeling/Preprocess.py", line 403, in mds
    mds_train.fit(train)
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/site-packages/sklearn/manifold/_mds.py", line 488, in fit
    self.fit_transform(X, init=init)
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/site-packages/sklearn/manifold/_mds.py", line 534, in fit_transform
    self.embedding_, self.stress_, self.n_iter_ = smacof(
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/site-packages/sklearn/manifold/_mds.py", line 285, in smacof
    results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/site-packages/joblib/parallel.py", line 1056, in __call__
    self.retrieve()
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/site-packages/joblib/parallel.py", line 935, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/site-packages/joblib/_parallel_backends.py", line 542, in wrap_future_result
    return future.result(timeout=timeout)
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/work/mmalekis/.conda/envs/gennn/lib/python3.8/concurrent/futures/_base.py", line 388, in __get_result
    raise self._exception
joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.

The exit codes of the workers are {SIGKILL(-9)}
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=4709.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
