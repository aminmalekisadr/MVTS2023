{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import io\n", "import urllib.request\n", "import os\n", "import zipfile\n", "import datetime\n", "import pandas as pd\n", "import math\n", "import random\n", "import forestci as fci\n", "import mlxtend\n", "from sklearn.metrics import zero_one_loss\n", "from sklearn.metrics import mean_squared_error\n", "from mlxtend.evaluate import bias_variance_decomp\n", "import tensorflow\n", "import time\n", "import gc\n", "import plotly.express as px\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Dense\n", "from tensorflow.keras.utils import plot_model\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.tree import export_graphviz  # with pydot\n", "from sklearn.metrics import confusion_matrix\n", "import warnings\n", "import logging\n", "my_devices = tensorflow.config.experimental.list_physical_devices(device_type='GPU')\n", "tensorflow.config.experimental.set_visible_devices(devices= my_devices, device_type='GPU')\n", "# To find out which devices your operations and tensors are assigned to\n", "#tensorflow.debugging.set_log_device_placement(True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pdb\n", "import matplotlib.pyplot as plt\n", "train_size_percentage = 0.9  # Training size\n", "mutation_rate = 0.1  # Mutation rate for GA\n", "min_mutation_momentum = 0.0001  # Min mutation momentum\n", "max_mutation_momentum = 0.1  # Max mutation momentum\n", "min_population = 3  # Min population for GA\n", "max_population = 10  # Max population for GA\n", "num_Iterations = 1 # Number of iterations to evaluate GA\n", "look_back = 500 # Num of time spaces to look back for training and testing\n", "max_dropout = 0.2  # Maximum percentage of dropout\n", "min_num_layers = 1  # Min number of hidden layers\n", "max_num_layers = 10  # Max number of hidden layers\n", "min_num_neurons = 10  # Min number of neurons in hidden layers\n", "max_num_neurons = 100 # Max number of neurons in hidden layers\n", "min_num_estimators = 100  # Min number of random forest trees\n", "max_num_estimators = 500  # Max number of random forest trees\n", "force_gc = True  # Forces garbage collector\n", "rnn_epochs = 100  # Epochs for RNN\n", "look_back2=50\n", "optimisers = ['SGD' ,'Adam']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rnn_types = ['LSTM', 'GRU', 'SimpleRNN']\n", "warnings.filterwarnings(\"ignore\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimisers = ['Adam']\n", "im=0\n", "rnn_types = ['LSTM', 'GRU', 'SimpleRNN']\n", "warnings.filterwarnings(\"ignore\")\n", "precision=[]\n", "recall=[]\n", "Accuracy=[]\n", "F1=[]\n", "force_gc=True\n", "def plot_confusion_matrix(cm, classes,\n", "                          normalize=False,\n", "                          title='Confusion matrix',\n", "                          cmap=plt.cm.Blues):\n", "    \"\"\"\n", "    This function prints and plots the confusion matrix.\n", "    Normalization can be applied by setting `normalize=True`.\n", "    \"\"\"\n", "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n", "    plt.title(title)\n", "    plt.colorbar()\n", "    tick_marks = np.arange(len(classes))\n", "    plt.xticks(tick_marks, classes, rotation=45)\n", "    plt.yticks(tick_marks, classes)\n", "    if normalize:\n", "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n", "        print(\"Normalized confusion matrix\")\n", "    else:\n", "        print('Confusion matrix, without normalization')\n", "    print(cm)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_df(data, start=0):\n", "    #    pdb.set_trace()\n", "    index = np.array(range(start, start + len(data)))\n", "    timestamp = index * 86400 + 1022819200\n", "    return pd.DataFrame({'timestamp': timestamp.astype(int), 'value': data[:, 0], 'index': index.astype(int)})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_sliding_window(data, sequence_length, stride=1):\n", "    X_list, y_list = [], []\n", "    for i in range(len(data)):\n", "        if (i + sequence_length) < len(data):\n", "            X_list.append(data.iloc[i:i + sequence_length:stride, :].values)\n", "            y_list.append(data.iloc[i + sequence_length, -1])\n", "    return np.array(X_list), np.array(y_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def inverse_transform(y):\n", "    return target_scaler.inverse_transform(y.reshape(-1, 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_dataset(dataset, look_back):\n", "    \"\"\"\n", "    Converts an array of values into a dataset matrix\n", "    :param dataset:\n", "    :param look_back:\n", "    :return:\n", "    \"\"\"\n", "    dataX, dataY = [], []\n", "    for i in range(len(dataset) - look_back - 1):\n", "        a = dataset[i:(i + look_back), 0]\n", "        dataX.append(a)\n", "        dataY.append(dataset[i + look_back, 0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    collect_gc()\n", "    return np.array(dataX), np.array(dataY)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def collect_gc():\n", "    \"\"\"\n", "    Forces garbage collector\n", "    :return:\n", "    \"\"\"\n", "    if force_gc:\n", "        gc.collect()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_dataset(dataset_path):\n", "    \"\"\"\n", "    Loads a dataset with training and testing arrays\n", "    :param dataset_path:\n", "    :return:\n", "    \"\"\"\n", "    # Load dataset\n", "    dataset = pd.read_csv(dataset_path, parse_dates=False, index_col=0)\n", "    dataset=np.array(dataset.value)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #dataset = dataset.value  # as numpy array\n", "    dataset = dataset.astype('float64')\n", "    #pdb.set_trace()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Normalise the dataset\n", "    scaler = MinMaxScaler(feature_range=(-1, 1))\n", "    dataset = scaler.fit_transform(dataset.reshape(-1,1))\n", "    train_size_percentage=0.7"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # split into train and test sets\n", "    train_size = int(len(dataset) * train_size_percentage)\n", "    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n", "    # reshape into X=t and Y=t+1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    train_x, train_y = create_dataset(train, look_back)\n", "    test_x, test_y = create_dataset(test, look_back)\n", "    # reshape input to be [samples, time steps, features]\n", "    train_x_rf, train_y_rf = create_dataset(train, look_back2)\n", "    test_x_rf, test_y_rf = create_dataset(test, look_back2)\n", "    train_x_rf_stf=train_x_rf.reshape(train_x_rf.shape[0],train_x_rf.shape[1],1)\n", "    test_x_rf_stf=test_x_rf.reshape(test_x_rf.shape[0],test_x_rf.shape[1],1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    train_x_stf = np.reshape(train_x, (train_x.shape[0],1, train_x.shape[1]))\n", "    test_x_stf = np.reshape(test_x, (test_x.shape[0],1, test_x.shape[1]))\n", "    train_x_st = np.reshape(train_x, (train_x.shape[0], train_x.shape[1]))\n", "    test_x_st = np.reshape(test_x, (test_x.shape[0], test_x.shape[1]))\n", "    #pdb.set_trace()\n", "    train_x_rf_st = np.reshape(train_x_rf, (train_x_rf.shape[0], train_x_rf.shape[1]))\n", "    test_x_rf_st = np.reshape(test_x_rf, (test_x_rf.shape[0], test_x_rf.shape[1]))\n", "    return dataset, scaler, train_x_stf, train_x_st, train_y, test_x_stf, test_x_st, test_y, train_x_rf_stf, train_x_rf, train_y_rf, test_x_rf_stf, test_x_rf, test_y_rf, train_x_rf_st,test_x_rf_st"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_rnn(hidden_layers):\n", "    \"\"\"\n", "    Generates a RNN using an array of hidden layers including the number of neurons for each layer\n", "    :param hidden_layers:\n", "    :return:\n", "    \"\"\"\n\n", "    # Create and fit the RNN\n", "    model = Sequential()\n", "    # Add input layer\n", "    model.add(Dense(1, input_shape=(1, look_back)))\n", "    # pdb.set_trace()\n", "    # Add hidden layers\n", "    for i in range(len(hidden_layers)):\n", "        neurons_layer = hidden_layers[i]\n", "        # Randomly select rnn type of layer\n", "        rnn_type_index = random.randint(0, len(rnn_types) - 1)\n", "        rnn_type = rnn_types[rnn_type_index]\n", "        dropout = random.uniform(0, max_dropout)  # dropout between 0 and max_dropout\n", "        return_sequences = i < len(hidden_layers) - 1  # Last layer cannot return sequences when stacking\n\n", "        # Select and add type of layer\n", "        if rnn_type == 'LSTM':\n", "            model.add(LSTM(neurons_layer, dropout=dropout, return_sequences=return_sequences))\n", "        elif rnn_type == 'GRU':\n", "            model.add(GRU(neurons_layer, dropout=dropout, return_sequences=return_sequences))\n", "        elif rnn_type == 'SimpleRNN':\n", "            model.add(SimpleRNN(neurons_layer, dropout=dropout, return_sequences=return_sequences))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Add output layer\n", "    model.add(Dense(1))\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict_dist(X, model, num_samples):\n", "    preds = [model(X, training=True) for _ in range(num_samples)]\n", "    return np.hstack(preds)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict_point(X, model, num_samples):\n", "    pred_dist = predict_dist(X, model, num_samples)\n", "    return pred_dist.mean(axis=1),pred_dist.var(axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_rnn(model, train_x, test_x, train_y, test_y, scaler, optimiser,name):\n", "    \"\"\"\n", "    Evaluates the RNN model using the training and testing data\n", "    :param model:\n", "    :param train_x:\n", "    :param test_x:\n", "    :param train_y:\n", "    :param test_y:\n", "    :param scaler:\n", "    :param optimiser:\n", "    :return:\n", "    \"\"\"\n", "    model.compile(loss='mean_squared_error', optimizer=optimiser)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    model.fit(train_x, train_y, epochs=rnn_epochs,  shuffle=False, batch_size=128, verbose=2)\n", "    # Forecast\n", "  #  pdb.set_trace()\n", "    train_predict = model.predict(train_x)\n", "    test_predict = model.predict(test_x)\n", "    # Invert forecasts\n", "    train_MC_predict = predict_dist(train_x, model, num_samples=100)\n", "    test_MC_predict = predict_dist(test_x, model, num_samples=100)\n", "    train_MC_predict_point, train_MC_predict_var = predict_point(train_x, model, num_samples=100)\n", "    test_MC_predict_point, test_MC_predict_var = predict_point(test_x, model, num_samples=100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [" \n", "    train_predict = scaler.inverse_transform(train_predict)\n", "    train_y = scaler.inverse_transform([train_y])\n", "    test_y= scaler.inverse_transform([test_y])\n", "    test_predict= scaler.inverse_transform(test_predict)\n", "    train_MC_predict_point= scaler.inverse_transform([train_MC_predict_point])\n", "    test_MC_predict_point = scaler.inverse_transform([test_MC_predict_point])\n", "    test_uncertainty_df = pd.DataFrame()\n", "#    pdb.set_trace()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #test_uncertainty_df = test_uncertainty_df[['index',  'value_mean', 'value_std']]\n", "    test_uncertainty_df['value_mean']=test_MC_predict_point[0]\n", "    test_uncertainty_df['lower_bound'] = test_MC_predict_point[0] - 3 * abs(test_MC_predict_var)\n", "    test_uncertainty_df['upper_bound'] = test_MC_predict_point[0] + 3 * abs(test_MC_predict_var)\n", "    test_uncertainty_df['index'] = test_uncertainty_df.index\n", "    import plotly.graph_objects as go"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    test_uncertainty_plot_df = test_uncertainty_df#.copy(deep=True)\n", "    # test_uncertainty_plot_df = test_uncertainty_plot_df.loc[test_uncertainty_plot_df['date'].between('2016-05-01', '2016-05-09')]\n", "    truth_uncertainty_plot_df = pd.DataFrame()\n", "    truth_uncertainty_plot_df['value'] = test_y[0]\n", "    truth_uncertainty_plot_df['index'] = truth_uncertainty_plot_df.index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #.copy(deep=True)\n", "    # truth_uncertainty_plot_df = truth_uncertainty_plot_df.loc[testing_truth_df['date'].between('2016-05-01', '2016-05-09')]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    upper_trace = go.Scatter(\n", "        x=test_uncertainty_plot_df['index'],\n", "        y=test_uncertainty_plot_df['upper_bound'],\n", "        mode='lines',\n", "        fill=None,\n", "        name='99% Upper Confidence Bound'\n", "    )\n", "    lower_trace = go.Scatter(\n", "        x=test_uncertainty_plot_df['index'],\n", "        y=test_uncertainty_plot_df['lower_bound'],\n", "        mode='lines',\n", "        fill='tonexty',\n", "        name='99% Lower Confidence Bound',\n", "        fillcolor='rgba(255, 211, 0, 0.1)',\n", "    )\n", "    real_trace = go.Scatter(\n", "        x=truth_uncertainty_plot_df['index'],\n", "        y=truth_uncertainty_plot_df['value'],\n", "        mode='lines',\n", "        fill=None,\n", "        name='Real Values'\n", "    )\n", "    data = [upper_trace, lower_trace, real_trace]\n", "    fig = go.Figure(data=data)\n", "    fig.update_layout(title='Uncertainty MCDropout Test Data',\n", "                      xaxis_title='index',\n", "                      yaxis_title='value',\n", "                      legend_font_size=14,\n", "                      )\n", "    #fig.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    bounds_df = pd.DataFrame()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Using 99% confidence bounds\n", "    bounds_df['lower_bound'] = test_uncertainty_plot_df['lower_bound']\n", "    bounds_df['prediction'] = test_uncertainty_plot_df['value_mean']\n", "    bounds_df['real_value'] = truth_uncertainty_plot_df['value']\n", "    bounds_df['upper_bound'] = test_uncertainty_plot_df['upper_bound']\n", "    bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n", "                              (bounds_df['real_value'] <= bounds_df['upper_bound']))\n", "    print(\"Proportion of points contained within 99% confidence interval:\",\n", "          bounds_df['contained'].mean())\n", "    predictedanomaly = bounds_df.index[~bounds_df['contained']]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    test_predict = scaler.inverse_transform(test_predict)\n", "  #  test_y = scaler.inverse_transform([test_y])\n", "    # Calculate RMSE for train and test\n", "    train_score = mean_squared_error(train_y[0], train_predict[:, 0])\n", "    train_score_MC = mean_squared_error(train_y[0], train_MC_predict_point[0, :])\n", "    test_score_MC =mean_squared_error(test_y[0], test_MC_predict_point[0, :])\n", "    # print('Train Score: %.2f RMSE' % (train_score))\n", "    test_score = mean_squared_error(test_y[0], test_predict[:, 0])\n", "    # print('Test Score: %.2f RMSE' % (test_score))\n", "    #pdb.set_trace()\n", "    model.train_score = train_score\n", "    model.test_score = test_score\n", "    model.train_score_MC = train_score_MC\n", "    model.test_score_MC = test_score_MC\n", "    training_df=pd.DataFrame()\n", "    testing_df= pd.DataFrame()\n", "    training_truth_df=pd.DataFrame()\n", "    testing_truth_df=pd.DataFrame()\n", "    training_df['value']=train_MC_predict_point[0]\n", "    training_df['index'] = training_df.index\n", "    training_df['source'] = 'Training Prediction'\n", "    testing_df['value']=test_MC_predict_point[0]\n", "    testing_df['index'] = testing_df.index\n", "    testing_df['source'] = 'Test Prediction'\n", "    training_truth_df['value'] = train_y[0]\n", "    training_truth_df['index'] = training_truth_df.index\n", "    training_truth_df['source'] = 'True Value Training'\n", "    testing_truth_df['value'] = test_y[0]\n", "    testing_truth_df['index'] = testing_truth_df.index\n", "    testing_truth_df['source'] = 'True Value Testing'\n", "    #pdb.set_trace()\n", "    evaluation = pd.concat([training_df,\n", "                                testing_df,\n", "                                training_truth_df,\n", "                                testing_truth_df\n", "                                ], axis=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #fig = px.line(evaluation,\n", "     #             x=\"index\",\n", "      #            y=\"value\",\n", "       #           color=\"source\")\n", "    #fig.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    predictedanomaly = bounds_df.index[~bounds_df['contained']]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   # CSV_URL = 'https://github.com/khundman/telemanom/raw/master/labeled_anomalies.csv'\n\n", "    # %%\n", "    #os.makedirs('csv', exist_ok=True)\n\n", "    # %%\n", "    #selected_columns = ['index', 'name', 'value']\n\n", "    #df_label = pd.read_csv(CSV_URL)\n", "   # pdb.set_trace()\n", "    #label_row = df_label[df_label.chan_id == name]\n\n", "    #labels = label_row['anomaly_sequences'][label_row['anomaly_sequences'].index]\n\n", "    #appended_data = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   # labels = eval(labels[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   # for i in range(len(labels)):\n", "    #    anom = labels[i]\n", "     #   start = anom[0]\n", "      #  end = anom[1]\n", "       # index = np.array(range(start, end))\n\n", "        #timestamp = index * 86400 + 1022819200\n\n", "        #anomalies = pd.DataFrame({'timestamp': timestamp.astype(int), 'value': 1, 'index': index})\n", "        #appended_data.append(anomalies)\n\n", "    #label_data = pd.concat(appended_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   # label_data['name'] = name\n", "    #label_data = label_data[selected_columns]\n", "    # label_data .to_csv('csv/' + name + '.csv', index=False)\n\n", "    # name='F-7'\n", "  #  MSL = df_label[df_label.spacecraft == 'MSL']['chan_id']\n", "   # SMAP = df_label[df_label.spacecraft == 'SMAP']['chan_id']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    N = 15\n", "    newarr = []\n", "    for i in range(len(predictedanomaly) - N):\n", "        if (predictedanomaly[i] + 1 == predictedanomaly[i + 1] and predictedanomaly[i + 1] + 1 == predictedanomaly[\n", "            i + 2] and predictedanomaly[i + 3] + 1 == predictedanomaly[i + 4] and predictedanomaly[i + 4] + 1 ==\n", "                predictedanomaly[i + 5]\n", "                and predictedanomaly[i + 5] + 1 == predictedanomaly[i + 6] and predictedanomaly[i + 6] + 1 ==\n", "                predictedanomaly[i + 7]\n", "                and predictedanomaly[i + 7] + 1 == predictedanomaly[i + 8]\n", "                and predictedanomaly[i + 8] + 1 == predictedanomaly[i + 9]\n", "                and predictedanomaly[i + 9] + 1 == predictedanomaly[i + 10]\n", "                and predictedanomaly[i + 10] + 1 == predictedanomaly[i + 11]\n", "                and predictedanomaly[i + 11] + 1 == predictedanomaly[i + 12]\n", "                and predictedanomaly[i + 12] + 1 == predictedanomaly[i + 13]\n", "                and predictedanomaly[i + 13] + 1 == predictedanomaly[i + 14]\n", "                and predictedanomaly[i + 14] + 1 == predictedanomaly[i + 15]):\n", "            newarr.append(predictedanomaly[i])\n", "    #        newarr.append(predictedanomaly[i + 1])\n", "    #       newarr.append(predictedanomaly[i + 2])\n", "    predicteddanomaly = list(set(newarr))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   # realanomaly = label_data['index']\n", "    predicter = list(range(len(test_uncertainty_df)))\n\n", "    #a1 = pd.DataFrame(index=range(len(test_uncertainty_df)), columns=range(2))\n", "    #a1.columns = ['index', 'value']\n\n", "    #a2 = pd.DataFrame(index=range(len(test_uncertainty_df)), columns=range(2))\n", "    #a2.columns = ['index', 'value']\n\n", "    #for i in range(len(predicter)):\n", "     #   if i in predicteddanomaly:\n", "      #      a1.iloc[i, 1] = 1\n", "       # else:\n", "        #    a1.iloc[i, 1] = 0\n\n", "    #for i in range(len(predicter)):\n", "     #   if i in realanomaly:\n", "      #      a2.iloc[i, 1] = 1\n", "       # else:\n", "        #    a2.iloc[i, 1] = 0\n\n", "    #y_real = a2.value\n", "    #y_real = y_real.astype(int)\n", "    #y_predi = a1.value\n", "    #y_predi = y_predi.astype(int)\n\n", "    #cm = confusion_matrix(y_true=y_real, y_pred=y_predi)\n", "    #cm_plot_labels = ['no_anomaly', 'had_anomaly']\n", "    #plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n\n", "    # tp = len([np.where(predicteddanomaly == x)[0] for x in realanomaly])\n", "    # fp = len(predicteddanomaly) - tp\n", "    # fn = 0\n", "    # tn = len(truth_uncertainty_plot_df) - tp - fp - fn\n\n", "    #tp = cm[0][0]\n", "    #fp = cm[0][1]\n", "    #fn = cm[1][0]\n", "    #tn = cm[1][1]\n\n", "    #precision1 = tp / (tp + fp)\n", "    #recall1 = tp / (tp + fn)\n", "    #Accuracy1 = (tp + tn) / len(truth_uncertainty_plot_df)\n", "    #F11 = 2 / ((1 / precision1) + (1 / recall1))\n", "   # print('precision', precision1, 'Signal', name)\n", "   # print('recall', recall1, 'Signal', name)\n", "   # print('Accuracy', Accuracy1, 'Signal', name)\n", "   # print('F1', F11, 'Signal', name)\n", "    #precision.append(precision1)\n", "    #F1.append(F11)\n", "    #Accuracy.append(Accuracy1)\n", "    #recall.append(recall1)\n", "    model.train_score_MC=train_score_MC\n", "    model.test_score_MC=test_score_MC\n", "    model. train_score= train_score\n", "    model.test_score=test_score\n", "    model.train_predict=train_predict\n", "    model. test_predict= test_predict\n", "    model.test_MC_predict=test_MC_predict\n", "    model.test_MC_predict_point=test_MC_predict_point\n", "    model.train_MC_predict=train_MC_predict\n", "    model.train_MC_predict_point=train_MC_predict_point\n", "    model.test_MC_predict_var=test_MC_predict_var\n", "    model.train_MC_predict_var=train_MC_predict_var\n", "    model.predictedanomaly=predictedanomaly\n", "    return train_score_MC,  test_score_MC, train_score, test_score, train_predict, test_predict,test_MC_predict,test_MC_predict_point,train_MC_predict,train_MC_predict_point, test_MC_predict_var, train_MC_predict_var,predictedanomaly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def crossover_rnn(model_1, model_2):\n", "    \"\"\"\n", "    Executes crossover for the RNN in the GA for 2 models, modifying the first model\n", "    :param model_1:\n", "    :param model_2:\n", "    :return:\n", "    \"\"\"\n", "    # new_model = copy.copy(model_1)\n", "    new_model = model_1\n\n", "    # Probabilty of models depending on their RMSE test score\n", "    # Lower RMSE score has higher prob\n", "    test_score_total = model_1.test_score + model_2.test_score\n", "    model_1_prob = 1 - (model_1.test_score / test_score_total)\n", "    model_2_prob = 1 - model_1_prob\n", "    # Probabilities of each item for each model (all items have same probabilities)\n", "    model_1_prob_item = model_1_prob / (len(model_1.layers) - 2)\n", "    model_2_prob_item = model_2_prob / (len(model_2.layers) - 2)\n\n", "    # Number of layers of new generation depend on probability of each model\n", "    num_layers_new_gen = int(model_1_prob * (len(model_1.layers) - 1) + model_2_prob * (len(model_2.layers) - 1))\n\n", "    # Create list of int with positions of the layers of both models.\n", "    cross_layers_pos = []\n", "    # Create list of weights\n", "    weights = []\n", "    # Add positions of layers for model 1. Input and ouput layer are not added.\n", "    for i in range(2, len(model_1.layers)):\n", "        mod_item = type('', (), {})()\n", "        mod_item.pos = i\n", "        mod_item.model = 1\n", "        cross_layers_pos.append(mod_item)\n", "        weights.append(model_1_prob_item)\n\n", "    # Add positions of layers for model 2. Input and ouput layer are not added.\n", "    for i in range(2, len(model_2.layers)):\n", "        mod_item = type('', (), {})()\n", "        mod_item.pos = i\n", "        mod_item.model = 2\n", "        cross_layers_pos.append(mod_item)\n", "        weights.append(model_2_prob_item)\n", "    collect_gc()\n\n", "    # If new num of layers are larger than the num crossover layers, keep num of crossover layers\n", "    if num_layers_new_gen > len(cross_layers_pos):\n", "        num_layers_new_gen = len(cross_layers_pos)\n\n", "    # Randomly choose num_layers_new_gen layers of the new list\n", "    cross_layers_pos = list(np.random.choice(cross_layers_pos, size=num_layers_new_gen, replace=False, p=weights))\n\n", "    # Add both group of hidden layers to new group of layers using previously chosen layer positions of models\n", "    cross_layers = []\n", "    for i in range(len(cross_layers_pos)):\n", "        mod_item = cross_layers_pos[i]\n", "        if mod_item.model == 1:\n", "            cross_layers.append(model_1.layers[mod_item.pos])\n", "        else:\n", "            cross_layers.append(model_2.layers[mod_item.pos])\n", "    collect_gc()\n\n", "    # Add input layer randomly from parent 1 or parent 2\n", "    bit_random = random.randint(0, 1)\n", "    if bit_random == 0:\n", "        cross_layers.insert(0, model_1.layers[0])\n", "    else:\n", "        cross_layers.insert(0, model_2.layers[0])\n", "    bit_random = random.randint(0, 1)\n", "    if bit_random == 0:\n", "        cross_layers.append(model_1.layers[len(model_1.layers) - 1])\n", "    else:\n", "        cross_layers.append(model_2.layers[len(model_2.layers) - 1])\n\n", "    # Set new layers\n", "    new_model._layers = cross_layers\n", "    return new_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mutate_rnn(model):\n", "    \"\"\"\n", "    Mutates the RNN model\n", "    :param model:\n", "    :return:\n", "    \"\"\"\n", "    for i in range(len(model.layers)):\n", "        # Mutate randomly each layer\n", "        bit_random = random.uniform(0, 1)\n", "        if bit_random <= mutation_rate:\n", "            weights = model.layers[i].get_weights()  # list of weights as numpy arrays\n", "            # calculate mutation momentum\n", "            mutation_momentum = random.uniform(min_mutation_momentum, max_mutation_momentum)\n", "            new_weights = [x * mutation_momentum for x in weights]\n", "            model.layers[i].set_weights(new_weights)\n", "    collect_gc()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_plot_model_rnn(model):\n", "    \"\"\"\n", "    Saves the plot of the RNN model\n", "    :param model:\n", "    :return:\n", "    \"\"\"\n", "    plot_model(model, show_shapes=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_rf(estimators):\n", "    \"\"\"\n", "    Generates a Random Forest with the number of estimators to use\n", "    :param estimators:\n", "    :return:\n", "    \"\"\"\n", "    # Create and fit the RF"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    model = RandomForestRegressor(n_estimators=estimators, criterion='mse', max_depth=None, min_samples_split=2,\n", "                                  min_samples_leaf=4, max_features='auto', max_leaf_nodes=None, bootstrap=2,\n", "                                  oob_score=False, n_jobs=4, random_state=None, verbose=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_rf(model, train_x, test_x, train_y, test_y, scaler):\n", "    \"\"\"\n", "    Evaluates the Random Forest with training and testing data\n", "    :param model:\n", "    :param train_x:\n", "    :param test_x:\n", "    :param train_y:\n", "    :param test_y:\n", "    :param scaler:\n", "    :return:\n", "    \"\"\"\n", "    test_uncertainty_df=pd.DataFrame()\n\n", "    # Forecast"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["   # train_x = [train_x[i][0] for i in range(train_x.shape[0])]\n", "   # train_x=  np.array(train_x).reshape(-1,1)\n", "    train_x = np.array(train_x)\n", "   # test_x = [test_x[i][0] for i in range(test_x.shape[0])]\n", "    test_x = np.array(test_x)#.reshape(-1,1)\n", "    train_y = np.array(train_y)#.reshape(-1, 1)\n", "    test_y = np.array(test_y)#.reshape(-1, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #pdb.set_trace()\n", "    model.fit(train_x, train_y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    train_predict = model.predict(train_x)#.reshape(-1,1) #mpg x_hat)\n", "    test_predict = model.predict(test_x)#.reshape(-1,1)\n", "    #mpg y_hat)\n", "    train_predict = scaler.inverse_transform([train_predict])\n", "    train_y = scaler.inverse_transform([train_y])\n", "    test_predict = scaler.inverse_transform([test_predict])\n", "    test_y = scaler.inverse_transform([test_y])\n", "    # Calculate RMSE for train and test\n", "    n_experiments = 20\n", "    test_uncertainty_df = pd.DataFrame()\n", "    for i in range(n_experiments):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        model1 = RandomForestRegressor(n_estimators=model.n_estimators, criterion='mse', max_depth=None, min_samples_split=2,\n", "                                      min_samples_leaf=4, max_features='auto', max_leaf_nodes=None, bootstrap=2,\n", "                                      oob_score=False, n_jobs=4, random_state=None, verbose=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        model1.fit(train_x, train_y[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        experiment_predictions = model1.predict(test_x)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        test_uncertainty_df['value_{}'.format(i)] = scaler.inverse_transform([experiment_predictions])[0]\n", "        # y_test_predict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    log_energy_consumption_df = test_uncertainty_df.filter(like='value', axis=1)\n", "    test_uncertainty_df['value_mean'] = log_energy_consumption_df.mean(axis=1)\n", "    test_uncertainty_df['value_std'] = log_energy_consumption_df.std(axis=1)\n", "    #pdb.set_trace()\n\n", "    #variance_rf = fci.random_forest_error(model, train_x, test_x)\n", "    # Invert forecasts\n", "    # Plot error bars for predicted MPG using unbiased variance\n", "    #plt.errorbar(test_y[0],test_uncertainty_df['value_mean'].values, yerr=variance_rf, fmt='o')\n", "    #plt.show()\n", "    #test_uncertainty_df['value_std']=variance_rf\n", "#    test_uncertainty_df['value_mean'] = test_predict[0]\n", "    test_uncertainty_df['lower_bound'] = test_uncertainty_df['value_mean'] -3* test_uncertainty_df['value_std']\n", "    test_uncertainty_df['upper_bound'] =test_uncertainty_df['value_mean'] +3* test_uncertainty_df['value_std']\n", "    test_uncertainty_df['index']= pd.DataFrame(test_predict[0]).index\n", "    import plotly.graph_objects as go\n", "    test_uncertainty_plot_df = test_uncertainty_df  # .copy(deep=True)\n", "    # test_uncertainty_plot_df = test_uncertainty_plot_df.loc[test_uncertainty_plot_df['date'].between('2016-05-01', '2016-05-09')]\n", "    truth_uncertainty_plot_df = pd.DataFrame()\n", "    truth_uncertainty_plot_df['value'] = test_y[0]\n", "    truth_uncertainty_plot_df['index'] = truth_uncertainty_plot_df.index\n\n", "    # .copy(deep=True)\n", "    # truth_uncertainty_plot_df = truth_uncertainty_plot_df.loc[testing_truth_df['date'].between('2016-05-01', '2016-05-09')]\n", "    upper_trace = go.Scatter(\n", "        x=test_uncertainty_plot_df['index'],\n", "        y=test_uncertainty_plot_df['upper_bound'],\n", "        mode='lines',\n", "        fill=None,\n", "        name='99% Upper Confidence Bound   '\n", "    )\n", "    lower_trace = go.Scatter(\n", "        x=test_uncertainty_plot_df['index'],\n", "        y=test_uncertainty_plot_df['lower_bound'],\n", "        mode='lines',\n", "        fill='tonexty',\n", "        name='99% Lower Confidence Bound',\n", "        fillcolor='rgba(255, 211, 0, 0.1)',\n", "    )\n", "    real_trace = go.Scatter(\n", "        x=truth_uncertainty_plot_df['index'],\n", "        y=truth_uncertainty_plot_df['value'],\n", "        mode='lines',\n", "        fill=None,\n", "        name='Real Values'\n", "    )\n", "    data = [upper_trace, lower_trace, real_trace]\n", "    fig = go.Figure(data=data)\n", "    fig.update_layout(title='RF Uncertainty',\n", "                      xaxis_title='index',\n", "                      yaxis_title='value',\n", "                      legend_font_size=14,\n", "                      )\n", "    #fig.show()\n", "    bounds_df = pd.DataFrame()\n\n", "    # Using 99% confidence bounds\n", "    bounds_df['lower_bound'] = test_uncertainty_plot_df['lower_bound']\n", "    bounds_df['prediction'] = test_uncertainty_plot_df['value_mean']\n", "    bounds_df['real_value'] = truth_uncertainty_plot_df['value']\n", "    bounds_df['upper_bound'] = test_uncertainty_plot_df['upper_bound']\n", "    bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n", "                              (bounds_df['real_value'] <= bounds_df['upper_bound']))\n", "    print(\"Proportion of points contained within 99% confidence interval:\",\n", "          bounds_df['contained'].mean())\n", "    predictedanomaly = bounds_df.index[~bounds_df['contained']]\n", "    #pdb.set_trace()\n", "    train_y = scaler.inverse_transform(train_y)\n", "    test_predict = scaler.inverse_transform(test_predict)\n", "    test_y = scaler.inverse_transform(test_y)\n\n", "    #plt.errorbar(test_y, test_predict, yerr=np.sqrt(variance_rf.mean()), fmt='o')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    train_score = mean_squared_error(train_y[0], train_predict[0])\n", "    # print('Train Score: %.2f RMSE' % (train_score))\n", "    test_score = mean_squared_error(test_y[0], test_predict[0])\n", "    # print('Test Score: %.2f RMSE' % (test_score))\n", "    model.train_score = train_score\n", "    model.test_score = test_score\n", "    variance_rf = test_uncertainty_df['value_std'].values\n", "    model.variance= variance_rf\n", "    model.anomalies=predictedanomaly\n", "    model.test_predict=test_predict\n", "    model.train_predict = train_predict\n", "    return train_score, test_score, train_predict, test_predict, variance_rf, predictedanomaly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def crossover_rf(model_1, model_2):\n", "    \"\"\"\n", "    Executes crossover for the RF in the GA for 2 models, modifying the first model\n", "    :param model_1:\n", "    :param model_2:\n", "    :return:\n", "    \"\"\"\n", "    # new_model = copy.copy(model_1)\n", "    new_model = model_1\n\n", "    # Probabilty of models depending on their RMSE test score\n", "    test_score_total = model_1.test_score + model_2.test_score\n", "    model_1_prob = 1 - model_1.test_score / test_score_total\n", "    model_2_prob = 1 - model_1_prob\n\n", "    # New estimator is the sum of both estimators times their probability\n", "    new_model.n_estimators = math.ceil(model_1.n_estimators * model_1_prob + model_2.n_estimators * model_2_prob)\n", "    return new_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mutate_rf(model):\n", "    \"\"\"\n", "    Mutates the Random Forest\n", "    :param model:\n", "    :return:\n", "    \"\"\"\n", "    # Mutate randomly the estimator\n", "    bit_random = random.uniform(0, 1)\n", "    if bit_random <= mutation_rate:\n", "        # calculate mutation momentum\n", "        mutation_momentum = random.uniform(min_mutation_momentum, max_mutation_momentum)\n", "        # Mutate estimators\n", "        model.n_estimators = model.n_estimators + math.ceil(model.n_estimators * mutation_momentum)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_plot_model_rf(model):\n", "    \"\"\"\n", "    Saves the plot of the Random Forest model\n", "    :param model:\n", "    :return:\n", "    \"\"\"\n", "    for i in range(len(model.estimators_)):\n", "        estimator = model.estimators_[i]\n", "        out_file = open(\"trees/tree-\" + str(i) + \".dot\", 'w')\n", "        export_graphviz(estimator, out_file=out_file)\n", "        out_file.close()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ensemble_stacking(rnn_model,rf_model, model_1_values,best_rmse_rnn, model_2_values,best_rmse_rf, test, scaler,name):\n", "    \"\"\"\n", "    Ensemble result of 2 models using stacking and averaging.\n", "    Takes both model predictions, averages them and calculates the new RMSE\n", "    :param model_1_values:\n", "    :param model_2_values:\n", "    :return:\n", "    \"\"\"\n", "    var1=rnn_model.test_MC_predict_var\n", "    var2= rf_model.variance\n\n", "    # Generates the stacking values by averaging both predictions\n", "    stacking_values = []\n", "    stacking_values_uncertainty=[]\n", "    var_total=[]\n", "    for i in range(len(model_1_values)):\n", "        w1 = abs(1 / var1[i])\n", "        w2 = abs(1 / var2[i])\n", "        stacking_values.append((model_1_values[i][0] + model_2_values[0][i]) / 2)\n", "        stacking_values_uncertainty.append((model_1_values[i][0]*w1+ model_2_values[0][i]*w2 )/(w1+w2))\n", "        var_total.append(1/(w1+w2))\n", "   # pdb.set_trace()\n", "    test = scaler.inverse_transform([test])\n", "    rmse1 =mean_squared_error(test[0], stacking_values)\n", "    rmse2 = mean_squared_error(test[0], stacking_values_uncertainty)\n", "    test_uncertainty_df=pd.DataFrame()\n", "    #pdb.set_trace()\n", "    test_uncertainty_df['lower_bound'] = np.array(stacking_values_uncertainty) - 3 *  np.array(var_total)\n", "    test_uncertainty_df['upper_bound'] = np.array(stacking_values_uncertainty)+ 3 * np.array(var_total)\n", "    test_uncertainty_df['index'] = pd.DataFrame(test[0]).index.values\n", "    import plotly.graph_objects as go\n", "    test_uncertainty_plot_df = test_uncertainty_df  # .copy(deep=True)\n", "    # test_uncertainty_plot_df = test_uncertainty_plot_df.loc[test_uncertainty_plot_df['date'].between('2016-05-01', '2016-05-09')]\n", "    truth_uncertainty_plot_df = pd.DataFrame()\n", "    truth_uncertainty_plot_df['value'] = test[0]\n", "    truth_uncertainty_plot_df['index'] = truth_uncertainty_plot_df.index\n\n", "    # .copy(deep=True)\n", "    # truth_uncertainty_plot_df = truth_uncertainty_plot_df.loc[testing_truth_df['date'].between('2016-05-01', '2016-05-09')]\n", "    upper_trace = go.Scatter(\n", "        x=test_uncertainty_plot_df['index'],\n", "        y=test_uncertainty_plot_df['upper_bound'],\n", "        mode='lines',\n", "        fill=None,\n", "        name='99% Upper Confidence Bound   '\n", "    )\n", "    lower_trace = go.Scatter(\n", "        x=test_uncertainty_plot_df['index'],\n", "        y=test_uncertainty_plot_df['lower_bound'],\n", "        mode='lines',\n", "        fill='tonexty',\n", "        name='99% Lower Confidence Bound',\n", "        fillcolor='rgba(255, 211, 0, 0.1)',\n", "    )\n", "    real_trace = go.Scatter(\n", "        x=truth_uncertainty_plot_df['index'],\n", "        y=truth_uncertainty_plot_df['value'],\n", "        mode='lines',\n", "        fill=None,\n", "        name='Real Values'\n", "    )\n", "    data = [upper_trace, lower_trace, real_trace]\n", "    fig = go.Figure(data=data)\n", "    fig.update_layout(title='RF Uncertainty',\n", "                      xaxis_title='index',\n", "                      yaxis_title='value',\n", "                      legend_font_size=14,\n", "                      )\n", "    #fig.show()\n", "    bounds_df = pd.DataFrame()\n\n", "    #pdb.set_trace()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Using 99% confidence bounds\n", "    bounds_df['lower_bound'] = test_uncertainty_plot_df['lower_bound']\n", "    bounds_df['prediction'] = stacking_values_uncertainty\n", "    bounds_df['real_value'] = truth_uncertainty_plot_df['value']\n", "    bounds_df['upper_bound'] = test_uncertainty_plot_df['upper_bound']\n", "    bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n", "                              (bounds_df['real_value'] <= bounds_df['upper_bound']))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["  #  print(\"Proportion of points contained within 99% confidence interval:\",\n", "   #       bounds_df['contained'].mean())\n", "    predictedanomaly = bounds_df.index[~bounds_df['contained']]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #train_y = scaler.inverse_transform(train_y)\n", "    #test_predict = scaler.inverse_transform(test_predict)\n", "    #test_y = scaler.inverse_transform(test_y)\n\n", "    # plt.errorbar(test_y, test_predict, yerr=np.sqrt(variance_rf.mean()), fmt='o')\n", "    #CSV_URL = 'https://github.com/khundman/telemanom/raw/master/labeled_anomalies.csv'\n\n", "    # %%\n", "    #os.makedirs('csv', exist_ok=True)\n\n", "    # %%\n", "    #selected_columns = ['index', 'name', 'value']\n\n", "    #df_label = pd.read_csv(CSV_URL)\n", "    # pdb.set_trace()\n", "    #label_row = df_label[df_label.chan_id == name]\n\n", "    #labels = label_row['anomaly_sequences'][label_row['anomaly_sequences'].index]\n\n", "    #appended_data = []\n\n", "    #labels = eval(labels[0])\n\n", "    #for i in range(len(labels)):\n", "     #   anom = labels[i]\n", "      #  start = anom[0]\n", "       # end = anom[1]\n\n", "        #index = np.array(range(start, end))\n\n", "        #timestamp = index * 86400 + 1022819200\n\n", "        #anomalies = pd.DataFrame({'timestamp': timestamp.astype(int), 'value': 1, 'index': index})\n", "        #appended_data.append(anomalies)\n\n", "    #label_data = pd.concat(appended_data)\n\n", "    #label_data['name'] = name\n", "    #label_data = label_data[selected_columns]\n", "    # label_data .to_csv('csv/' + name + '.csv', index=False)\n\n", "    # name='F-7'\n", "    #  MSL = df_label[df_label.spacecraft == 'MSL']['chan_id']\n", "    # SMAP = df_label[df_label.spacecraft == 'SMAP']['chan_id']\n", "    N = 15\n", "    newarr = []\n", "    for i in range(len(predictedanomaly) - N):\n", "        if (predictedanomaly[i] + 1 == predictedanomaly[i + 1] and predictedanomaly[i + 1] + 1 == predictedanomaly[\n", "            i + 2] and predictedanomaly[i + 3] + 1 == predictedanomaly[i + 4] and predictedanomaly[i + 4] + 1 ==\n", "                predictedanomaly[i + 5]\n", "                and predictedanomaly[i + 5] + 1 == predictedanomaly[i + 6] and predictedanomaly[i + 6] + 1 ==\n", "                predictedanomaly[i + 7]\n", "                and predictedanomaly[i + 7] + 1 == predictedanomaly[i + 8]\n", "                and predictedanomaly[i + 8] + 1 == predictedanomaly[i + 9]\n", "                and predictedanomaly[i + 9] + 1 == predictedanomaly[i + 10]\n", "                and predictedanomaly[i + 10] + 1 == predictedanomaly[i + 11]\n", "                and predictedanomaly[i + 11] + 1 == predictedanomaly[i + 12]\n", "                and predictedanomaly[i + 12] + 1 == predictedanomaly[i + 13]\n", "                and predictedanomaly[i + 13] + 1 == predictedanomaly[i + 14]\n", "                and predictedanomaly[i + 14] + 1 == predictedanomaly[i + 15]):\n", "            newarr.append(predictedanomaly[i])\n", "    #        newarr.append(predictedanomaly[i + 1])\n", "    #       newarr.append(predictedanomaly[i + 2])\n", "    predicteddanomaly = list(set(newarr))\n\n", "    #realanomaly = label_data['index']\n", "    predicter = list(range(len(test_uncertainty_df)))\n\n", "    #a1 = pd.DataFrame(index=range(len(test_uncertainty_df)), columns=range(2))\n", "    #a1.columns = ['index', 'value']\n\n", "    #a2 = pd.DataFrame(index=range(len(test_uncertainty_df)), columns=range(2))\n", "    #a2.columns = ['index', 'value']\n\n", "    #for i in range(len(predicter)):\n", "     #   if i in predicteddanomaly:\n", "      #      a1.iloc[i, 1] = 1\n", "      #  else:\n", "       #     a1.iloc[i, 1] = 0\n\n", "    #for i in range(len(predicter)):\n", "     #   if i in realanomaly:\n", "      #      a2.iloc[i, 1] = 1\n", "       # else:\n", "        #    a2.iloc[i, 1] = 0\n\n", "    #y_real = a2.value\n", "    #y_real = y_real.astype(int)\n", "    #y_predi = a1.value\n", "    #y_predi = y_predi.astype(int)\n\n", "    #cm = confusion_matrix(y_true=y_real, y_pred=y_predi)\n", "    #cm_plot_labels = ['no_anomaly', 'had_anomaly']\n", "    #plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')\n\n", "    # tp = len([np.where(predicteddanomaly == x)[0] for x in realanomaly])\n", "    # fp = len(predicteddanomaly) - tp\n", "    # fn = 0\n", "    # tn = len(truth_uncertainty_plot_df) - tp - fp - fn\n\n", "    #tp = cm[0][0]\n", "    #fp = cm[0][1]\n", "    #fn = cm[1][0]\n", "    #tn = cm[1][1]\n\n", "    #precision1 = tp / (tp + fp)\n", "    #recall1 = tp / (tp + fn)\n", "    #Accuracy1 = (tp + tn) / len(truth_uncertainty_plot_df)\n", "    #F11 = 2 / ((1 / precision1) + (1 / recall1))\n", "    #print('precision', precision1, 'Signal', name)\n", "    #print('recall', recall1, 'Signal', name)\n", "    #print('Accuracy', Accuracy1, 'Signal', name)\n", "    #print('F1', F11, 'Signal', name)\n", "    #precision.append(precision1)\n", "    #F1.append(F11)\n", "    #Accuracy.append(Accuracy1)\n", "    #recall.append(recall1)\n", "   # model.train_score_MC = train_score_MC\n", "   # model.test_score_MC = test_score_MC\n", "   # model.train_score = train_score\n", "   # model.test_score = test_score\n", "   # model.train_predict = train_predict\n", "    #model.test_predict = test_predict\n", "    #model.test_MC_predict = test_MC_predict\n", "    #model.test_MC_predict_point = test_MC_predict_point\n", "    #model.train_MC_predict = train_MC_predict\n", "    #model.train_MC_predict_point = train_MC_predict_point\n", "    #model.test_MC_predict_var = test_MC_predict_var\n", "    #model.train_MC_predict_var = train_MC_predict_var\n", "    #model.predictedanomaly = predictedanomaly"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    return stacking_values,  stacking_values_uncertainty, rmse1,rmse2,predicteddanomaly,var_total,var_total"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_ga(dataset):\n", "    \"\"\"\n", "    Evaluates and generates the ensemble model using Genetic Algorithms\n", "    :param dataset:\n", "    :return:\n", "    \"\"\"\n", "    print('#-----------------------------------------------')\n", "    print('  ', dataset)\n", "    print('#-----------------------------------------------')\n", "    dataset1=dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    dataset, scaler, train_x_stf, train_x_st, train_y, test_x_stf, test_x_st, test_y,train_x_rf_stf,train_x_rf, train_y_rf, test_x_rf_stf, test_x_rf, test_y_rf,train_x_rf_st, test_x_rf_st  = load_dataset(dataset)\n", "    start = time.time()  # Start Timer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    num_population = random.randint(min_population, max_population)  # Number of RNN to evaluate\n", "    # == 1) Generate initial population for RNN and Random Forest\n", "    population_rnn = []\n", "    population_rf = []\n", "    start_ga_1 = time.time()  # Start Timer\n", "    for i in range(num_population):\n", "        # -- RNN\n", "        # Generate random topology configuration\n", "        num_layers = random.randint(min_num_layers, max_num_layers)\n", "        hidden_layers = []\n", "        for j in range(num_layers):\n", "            num_neurons = random.randint(min_num_neurons, max_num_neurons)\n", "            hidden_layers.append(num_neurons)\n\n", "        # collect_gc()\n\n", "        # Generate and add rnn model to population\n", "        model_rnn = generate_rnn(hidden_layers)\n", "        population_rnn.append(model_rnn)\n\n", "        # -- RF\n", "        # Generate random number of estimators for RF\n", "        num_estimators = random.randint(min_num_estimators, max_num_estimators)\n\n", "        # Generate and add rf model to population\n", "        model_rf = generate_rf(num_estimators)\n", "        population_rf.append(model_rf)\n", "    end_ga_1 = time.time() - start_ga_1  # End Timer\n", "    print('Generate Initial population Time_Taken:%.3f' % end_ga_1)\n", "    collect_gc()\n", "    # print(len(population))\n", "    best_rmse_rnn = float(\"inf\")\n", "    best_rmse_rf = float(\"inf\")\n", "    best_rnn_model = None\n", "    best_test_predict_rnn = None\n", "    best_rf_model = None\n", "    best_test_predict_rf = None\n", "    # Evaluate fitness for\n", "    for i in range(num_Iterations):\n", "        print('=================================================================================================')\n", "        print(' iteration: %d, total iterations: %d, population size: %d ' % (i + 1, num_Iterations, num_population))\n", "        print('=================================================================================================')\n", "        # train_score, test_score = float(\"inf\"), float(\"inf\")\n", "        # == 2)  Evaluate fitness for population\n", "        start_ga_2 = time.time()  # Start Timer\n", "        for j in range(num_population):\n", "            # Evaluate fitness for RNN\n", "            rnn_model = population_rnn[j]\n", "            train_MC_score_rnn, test_MC_score_rnn,  train_score_rnn, test_score_rnn, train_predict_rnn, test_predict_rnn,\\\n", "            test_MC_dist_predict_rnn,test_MC_predict_point_rnn,train_MC_dist_predict_rnn, train_MC_predict_point_rnn,test_MC_predict_var_rnn,\\\n", "            train_MC_predict_var_rnn,anomalies_rnn= evaluate_rnn(rnn_model, train_x_stf,\n", "                                                                                         test_x_stf, train_y,\n", "                                                                                                test_y, scaler,\n", "                                                                                                optimisers[0],dataset1[7:-4])\n", "#            pdb.set_trace()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            # print('test predictions RNN: ', test_predict_rnn)\n", "            print('test_score RMSE RNN:%.3f ' % test_score_rnn)\n", "            print('test_score RMSE RNN_MC:%.3f ' % test_MC_score_rnn)\n", "            if test_score_rnn < best_rmse_rnn:\n", "                best_rmse_rnn = test_score_rnn\n", "                # best_rnn_model = copy.copy(rnn_model)\n", "                best_rnn_model = rnn_model\n", "                best_test_predict_rnn = test_predict_rnn\n", "                best_train_MC_score_rnn=train_MC_score_rnn\n", "                best_test_MC_score_rnn=test_MC_score_rnn\n", "                best_train_score_rnn=train_score_rnn\n", "                best_test_score_rnn=test_score_rnn\n", "                best_train_predict_rnn=train_predict_rnn\n", "                best_test_MC_dist_predict_rnn=test_MC_dist_predict_rnn\n", "                best_test_MC_predict_point_rnn=test_MC_predict_point_rnn\n", "                best_train_MC_dist_predict_rnn=train_MC_dist_predict_rnn\n", "                best_train_MC_predict_point_rnn=train_MC_predict_point_rnn\n", "                best_test_MC_predict_var_rnn=test_MC_predict_var_rnn\n", "                best_train_MC_predict_var_rnn=train_MC_predict_var_rnn\n", "                best_anomalies_rnn=anomalies_rnn\n\n", "            # Evaluate fitness for RF\n", "            rf_model = population_rf[j]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            train_score_rf, test_score_rf, train_predict_rf, test_predict_rf,variance_rf,anomalies_rf = evaluate_rf(rf_model, train_x_rf_st,\n", "                                                                                           test_x_rf_st, train_y_rf, test_y_rf,\n", "                                                                                           scaler)\n\n", "            # print('test predictions RF: ', test_predict_rf)\n", "            print('test_score RMSE RF:%.3f ' % test_score_rf)\n", "            if test_score_rf < best_rmse_rf:\n", "                best_rmse_rf = test_score_rf\n", "                # best_rf_model = copy.copy(rf_model)\n", "                best_rf_model = rf_model\n", "                best_test_predict_rf = rf_model.test_predict\n", "                best_train_score_rf=rf_model.train_score\n", "                best_test_score_rf=rf_model.test_score\n", "                best_train_predict_rf=rf_model.train_predict\n", "                best_test_predict_rf=rf_model.test_predict\n", "                best_variance_rf=rf_model.variance\n", "                best_anomalies_rf=rf_model.anomalies"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        end_ga_2 = time.time() - start_ga_2  # End Timer\n", "        print('Evaluate Fitness population Time_Taken:%.3f' % end_ga_2)\n", "        collect_gc()\n", "        print('Temporal Best RMSE RNN:%.3f' % best_rmse_rnn)\n", "        print('Temporal Best predictions: ', [x[0] for x in best_test_predict_rnn])\n", "        print('Temporal Best RMSE RF:%.3f' % best_rmse_rf)\n", "        print('Temporal Best predictions: ', [x for x in best_test_predict_rf])\n\n", "        # == 3) Create new population with new generations\n", "        # Every generation will use the current best RNN and best RF to mate\n", "        start_ga_3 = time.time()  # Start Timer\n", "        for pop_index in range(num_population):\n", "            # Select parents for mating\n", "            # Element at pop_index as parent. This will be replaced with the new generation\n", "            rnn_model_1 = population_rnn[pop_index]\n", "            rf_model_1 = population_rf[pop_index]\n\n", "            # 2 parent is the best found so far\n", "            rnn_model_2 = best_rnn_model\n", "            rf_model_2 = best_rf_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            # == 4) Create new generation with crossover\n", "            new_rnn_model = crossover_rnn(rnn_model_1, rnn_model_2)\n", "            new_rf_model = crossover_rf(rf_model_1, rf_model_2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            # == 5) Mutate new generation\n", "            mutate_rnn(new_rnn_model)\n", "            mutate_rf(new_rf_model)\n\n", "            # Replace current model in population\n", "            population_rnn[pop_index] = new_rnn_model\n", "            population_rf[pop_index] = new_rf_model\n", "        end_ga_3 = time.time() - start_ga_3  # End Timer\n", "        print('Generate new population Time_Taken:%.3f' % end_ga_3)\n", "        collect_gc()\n", "    collect_gc()\n", "    end = time.time() - start  # End Timer\n", "    # pdb.set_trace()\n", "    print('=============== BEST RNN ===============')\n", "    print('Best predictions: ', [x[0] for x in best_test_predict_rnn])\n", "    print('Best RMSE:%.3f Time_Taken:%.3f' % (best_rmse_rnn, end))\n", "    # save_plot_model_rnn(best_rnn_model)\n", "    print('=============== BEST RF ===============')\n", "    print('Best predictions: ', [x for x in best_test_predict_rf])\n", "    print('Best RMSE:%.3f Time_Taken:%.3f' % (best_rmse_rf, end))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # save_plot_model_rf(best_rf_model)\n", "    # print(best_rf_model.get_params(deep=True))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Ensemble\n", "    print('=============== Ensemble ===============')\n", "    # pdb.set_trace()\n", "    averaging_values, stacking_values_uncertainty, rmse1,rmse2,anomalies_merged,var_merge,var_total_uncertainty= ensemble_stacking(best_rnn_model,best_rf_model,best_test_predict_rnn,best_rmse_rnn, best_test_predict_rf,best_rmse_rf, test_y, scaler,dataset1[7:-4])\n", "   # print('Ensemble averaging_values: ', averaging_values)\n", "  #  print('Ensemble uncertainty_values: ', stacking_values_uncertainty)\n", "    print('Ensemble rmse: ', rmse1)\n", "    print('Ensemble rmse uncertainty: ', rmse2)\n", "    return best_anomalies_rf,best_anomalies_rnn,anomalies_merged,var_merge,best_variance_rf, best_test_MC_predict_var_rnn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_bptt(dataset):\n", "    \"\"\"\n", "    Evaluates and generates a RNN model using BPTT\n", "    :param dataset:\n", "    :return:\n", "    \"\"\"\n", "    print('#-----------------------------------------------')\n", "    print('  ', dataset)\n", "    print('#-----------------------------------------------')\n", "    dataset, scaler, train_x_stf, train_x_st, train_y, test_x_stf, test_x_st, test_y = load_dataset(dataset)\n", "    start = time.time()  # Start Timer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Generate a 1 hidden layer configuration\n", "    hidden_layers = [1]\n", "    # Generate and add rnn model to population\n", "    model_rnn = generate_rnn(hidden_layers)\n", "   # pdb.set_trace()\n", "    train_score_rnn, test_score_rnn, train_predict_rnn, test_predict_rnn = evaluate_rnn(model_rnn, train_x_stf,\n", "                                                                                        test_x_stf, train_y,\n", "                                                                                        test_y, scaler,\n", "                                                                                        optimisers[0],name)\n", "    end = time.time() - start  # End Timer\n", "    print('Predictions: ', [x[0] for x in test_predict_rnn])\n", "    print('RMSE:%.3f Time_Taken:%.3f' % (test_score_rnn, end))\n", "    # save_plot_model_rnn(model_rnn)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}